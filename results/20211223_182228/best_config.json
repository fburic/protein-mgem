{
 "attention_probs_dropout_prob": 0.0,
 "batch_size": 32,
 "hidden_act": "relu",
 "hidden_dropout_prob": 0.0,
 "hidden_size": 1024,
 "intermediate_size": 3072,
 "learning_rate": 2.5681310045153124e-07,
 "num_attention_heads": 4,
 "num_hidden_layers": 8,
 "num_train_epochs": 500,
 "vocab_size": 30,
 "type_vocab_size": 2,
 "initializer_range": 0.02,
 "layer_norm_eps": 1e-12,
 "max_position_embeddings": 1024,
 "gradient_accumulation_steps": 16,
 "patience": 50
}